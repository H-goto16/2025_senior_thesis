\documentclass[senior,final,11pt,dvipdfmx]{iscs-thesis}
%\documentclass[master,final,11pt]{iscs-thesis}
\usepackage{indentfirst}
\usepackage{amsmath}
\usepackage[dvipdfmx]{graphicx}
\usepackage[dvipdfmx]{color}
\usepackage{subfigure}
\usepackage{longtable}
\usepackage{url}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{float}
\usepackage{acronym}
\usepackage{amssymb}
\usepackage{algpseudocode}
\usepackage{algorithm}
\renewcommand{\algorithmicrequire}{Input}
\renewcommand{\algorithmicensure}{Output}
\usepackage[numbers,sort&compress]{natbib}
\usepackage{pdfpages}
\usepackage[dvipdfmx]{hyperref}
\usepackage{pxjahyper}

% 略語定義
\newacro{Obj}[物体検出]{Object Detection}
\newacro{Cla}[物体認識]{Classification}
\newacro{Seg}[領域分割]{Semantic Segmentation}
\newacro{Ins}[インスタンスセグメンテーション]{Instance Segmentation}
\newacro{Sem}[意味的領域分割]{Semantic Segmentation}
\newacro{Pan}[パノプティックセグメンテーション]{Panoptic Segmentation}
\newacro{CNN}[CNN]{畳み込みニューラルネットワーク}
\newacro{DSConv}[Depthwise Separable Conv]{Depthwise Separable Convolution}
\newacro{DConv}[Depthwise Conv]{Depthwise Convolution}
\newacro{PConv}[Pointwise Conv]{Pointwise Convolution}
\newacro{SE-Module}[SE-Module]{Squeeze-and-Excitation-Module}
\newacro{HS}[HS]{HardSwish}
\newacro{NAS}[NAS]{Neural Architecture Search}
\newacro{MBConv}[MBConv]{Mobile Inverted Residual Block}
\newacro{FLOPs}[FLOPs]{FLoating-point Operations}
\newacro{UpConv}[転置畳み込み]{Transposed Convolution}
\newacro{ASPP}[ASPP]{Atrous Spatial Pyramid Pooling}
\newacro{PPM}[PPM]{Pyramid Pooling Module}
\newacro{SimOTA}[SimOTA]{Simplified Optimal Transport Assignment}
\newacro{BCE}[BCE]{Binary Cross Entropy}
\newacro{IoU}[IoU]{Intersection over Union}
\newacro{AP}[AP]{Average Precision}
\newacro{NMS}[NMS]{Non-Maximum Suppression}
\usepackage[top=3.0cm, bottom=3.0cm, left=3.0cm, right=3.0cm]{geometry}
\pagenumbering{roman}
\begin{document}
\begin{titlepage}
  \centering
  {\Large ２０２５年度\par}
  \vspace{8mm}
  {\LARGE 学 \quad 士 \quad 論 \quad 文\par}
  \vspace{16mm}
  {\Large 論題\par}
  \vspace{6mm}
  {\Large \textbf{一般ユーザ向け開放語彙物体検出のスマホ運用基盤\\— 事前語彙化と少量学習による反復改善 —}\par}
  \vspace{20mm}
  {\large 指導教員 \quad 孟 \; 林 \; 教授\par}
  \vspace{8mm}
  {\large 立命館大学 \; 理工学部 \; 電子情報工学科\par}
  \vspace{4mm}
  {\large 学籍番号 \; 2290220041-3\par}
  {\large 氏名 \; 後藤 \; 晴貴\par}
\end{titlepage}

\chapter*{\centering 論文要旨}
AIが広く普及した現代において、対話型AIは一般ユーザに浸透している一方、画像認識は依然として専門知識を要し、エンジニア主体の領域に留まっている。本研究は一般ユーザを主対象とし、データ収集からアノテーション、学習、評価、利用までを一貫して支援し、個々の利用状況に特化した画像認識モデルの調整・ファインチューニングを可能にするアプリケーションを開発した。本システムにより、プログラミング経験が乏しいユーザでも少量の自前データから用途特化モデルを反復的に改善でき、画像認識活用の敷居を下げる。ケースとして料理画像検出を対象に、UI内でのデータ収集・ラベリングから学習、精度の可視化までを一連の操作で完結できることを示し、一般ユーザによるモデルカスタマイズの実用可能性を示した。定量的には，語彙登録のみの検証で10/10件の成立を確認し，手動ラベリング50枚＋ファインチューニングによりmAP50=0.9235，mAP50-95=0.5188，Precision=0.9065，Recall=0.7754，テスト13枚に対する検出率69.2\%を得た。
\\

\frontmatter
\tableofcontents
\listoffigures
\listoftables
\mainmatter
%-------1章--------------------------------------------------------
\chapter{はじめに}
対話型AIの普及により、一般ユーザが自然言語によって高度な情報処理を日常的に活用する時代が到来した。一方で、画像認識をはじめとするコンピュータビジョン（CV）は、データ収集、アノテーション、学習・評価、運用を含む一連の工程が分断されやすく、専門的な知識・ツール群を横断的に扱う必要があることから、依然として一般ユーザにとって参入障壁が高い。特に、用途特化のモデルを自分で調整（チューニング／ファインチューニング）し、反復的に改善していくためには、学習用データの拡充、失敗の可視化、改善仮説の検証といった実務的ワークフローが不可欠である。

本研究では、こうしたギャップを解消し、一般ユーザが「自分の目的に合った画像認識モデル」を自力で構築・調整できる環境を提供することを目的として、エンドツーエンドのモデル管理アプリケーションを開発した。本システムは、スマートフォン／PCのいずれからでも利用可能なクロスプラットフォームUIと、高性能なWeb APIを組み合わせ、データ収集・ラベリング・学習・推論・評価・モデル運用までを一貫して支援する。具体的には、カメラやギャラリーからの画像取得、ドラッグ\&ドロップによる直感的なアノテーション、Ultralytics YOLOを用いたリアルタイム物体検出、学習の非同期実行と進捗監視、履歴の可視化、データセット分析、モデルの保存・切替といった機能を統合し、一般ユーザでも試行錯誤を通じてモデルを改善できる実用的なワークフローを実現した。

運用面では、限られた計算資源でも現実的に扱えるよう、学習をバックグラウンドで非同期実行し、UIをブロックしない設計とした。さらに、再現性と保守性を高めるため、APIの仕様を明確化し、環境構築と動作手順を統一した。これにより、ユーザは最小限の初期設定で環境を整え、反復的なモデル改善に集中できる。

適用領域としては、料理画像を例題に据え、器や料理種別に応じた検出のしやすさ、データの集め方、モデルの差し替えやクラス管理など、実務的な観点からの検討を行った。用途特化の小規模データから出発し、UI上でのアノテーションと学習、可視化により改善ポイントを特定しながら、ユーザ自身の目的に合わせたモデルを段階的に洗練させることが可能である。これにより、画像認識活用の敷居を下げ、一般ユーザ主導の"現場適合"モデルの創出を後押しする。

本稿の主な貢献を以下に示す。
\begin{itemize}
    \item データ収集から学習・評価・運用までを統合した一般ユーザ向けCVモデル管理アプリケーションの設計・実装
    \item クロスプラットフォームUI上での直感的ラベリングとリアルタイム物体検出の統合による反復改善の促進
    \item 学習の非同期実行、進捗・履歴・メトリクスの可視化、モデル管理機能の一体化による実用的ワークフローの提供
    \item Web/Android/iOSでのクロスプラットフォームUIの提供
\end{itemize}

本論文では2章で背景および関連研究について述べ、3章で提案システムの設計方針と機能構成を示す。4章では語彙登録のみでの検出成立性をスマートフォンから検証し、5章では手動ラベリングとファインチューニングによる検出成立性の検証結果を示す。6章ではまとめと今後の課題（軽量化・最適化、拡張可能性、運用上の安全性・信頼性など）について議論する。
\\
%-------2章--------------------------------------------------------
\chapter{関連研究}

\section{物体検出モデル}
従来の物体検出はCOCOなどの固定語彙（closed-set）を前提としており、学習時に定義したカテゴリのみに限定されるという制約がある。一方、実環境では「未学習カテゴリ」を含む開放語彙（open-vocabulary）への拡張が重要である。YOLO系列はBackbone・Neck・Headからなる一段（one-stage）検出器として高い効率を示してきたが、語彙の固定という制約が実用展開のボトルネックとなってきた。

\subsection{YOLO-World}
YOLO-Worldは、従来YOLOの効率性を維持しつつ、視覚−言語モデリングによって開放語彙検出を実現した検出器である\cite{yoloworld}。中核は、(1) 再パラメータ化（re-parameterization）可能なVision-Language PAN（RepVL-PAN）によりテキスト埋め込みと画像特徴の相互作用を組み込み、推論時には言語エンコーダを取り除いて事前語彙化（prompt-then-detect, offline vocabulary）したテキスト埋め込みをモデル重みに吸収すること（付録 B.1 式 (1) より）、(2) 検出データ・グラウンディング・画像テキストの3種データを統一的に扱う領域–テキスト対に基づく大規模事前学習、にある。

学習時は、ラベル付き検出データ、グラウンディングデータ、画像–テキスト対から得る擬似ボックスを併用し、領域–テキスト対比学習で視覚と言語の整合を強化する。推論時はテキストエンコーダを用いず、オフラインで事前計算した語彙埋め込みをRepVL-PANへ再パラメータ化して統合するため、実行時レイテンシを抑えつつ開放語彙に対応できる。

学習スキームとしては，領域−テキスト対に基づくコントラスト学習を大規模データで行う。実データ（Objects365等）に加え，CC3Mなどの画像テキストデータから，名詞抽出→擬似ボックス生成（GLIP等）→CLIPによる再スコアリングとNMS/閾値フィルタリングという自動ラベリングパイプラインで領域−テキスト対を構築し（例：NMS=0.5，スコア閾値=0.3），開放語彙能力を強化する。小型モデル（YOLO-World-S）に対しては，高品質アノテーションや適量の擬似ラベルを組み合わせることでゼロショット性能が向上することが示されている。

性能面では，LVISにおいて35.4 APかつV100上で52 FPSを達成し（TensorRTなし），同規模の既存手法に対して精度・速度のバランスで優位性を示す。また，学習後は事前語彙化によりカテゴリ埋め込みをモデル重みに取り込み，エッジ展開時のテキストエンコーダ依存を排除する。さらに，COCOのような固定語彙タスクへ移行する際は，言語条件付けを無効化することで従来YOLOと同等の運用効率で微調整可能である。総じて，YOLO-Worldは固定語彙検出と開放語彙検出の橋渡しを行い，汎用実応用（ゼロショット検出，参照物体検出，オープン語彙インスタンスセグメンテーション等）に適した現実的なデプロイ手段を与える。

\subsection{GLIP}
GLIP\cite{glip}は，言語で条件付けたボックス予測を可能にするGrounded Language-Image Pre-trainingを提案し，
検出とグラウンディングを統一的に学習することで開放語彙検出の基盤を築いた。

\subsection{Grounding DINO}
Grounding DINO\cite{groundingdino}は，DINO系の検出器にテキスト条件付けを組み込み，
高精度なフレーズグラウンディングとゼロショット検出を実現した。

\subsection{OWL-ViT}
OWL-ViT\cite{owlvit}はVision Transformerを用いてオープン語彙検出をシンプルに実装し，
任意語彙に対する柔軟なゼロショット検出を示した。

\subsection{Detic}
Detic\cite{detic}は画像レベル監督を活用し，大規模語彙に拡張可能な検出を実現した。

\subsection{RegionCLIP}
RegionCLIP\cite{regionclip}は，領域レベルでのCLIP事前学習を行い，
領域特徴と言語埋め込みの整合を高めて開放語彙検出を強化した。

\subsection{CLIP と開放語彙の基盤}
CLIP\cite{clip}は大規模画像—テキスト対で学習した表現を提供し，
多くの開放語彙検出器の言語埋め込みの基盤となっている。

\section{食事画像データセット}
本研究の料理画像応用に関連して，以下のデータセットが広く用いられる。
\begin{itemize}
  \item Food-101\cite{food101}: 101クラスの料理画像分類データセット。
  \item FoodSeg103/UEC-FoodPix\cite{foodseg103}: 食事セグメンテーションのためのピクセル精度アノテーション。
  \item IM2Calories\cite{im2calories}, Nutrition5k\cite{nutrition5k}: カロリー・栄養推定の文脈での食事分析。
\end{itemize}

%-------3章--------------------------------------------------------
\chapter{提案手法}
本研究の目的は、スマートフォンを含む汎用端末のみでデータ収集からラベリング、学習、評価、運用までを一気通貫に反復できる実用システムを構築し、非専門家でも短時間で自作の用途特化モデルを育てられることを示す点にある。中核には開放語彙検出器であるYOLO-World\cite{yoloworld}を据え、事前語彙化によって実行時の言語エンコーダ依存を排除し、軽量・高速な推論を維持する。

\section{設計方針と構成}
フロントエンドはReact Native + ExpoによりAndroid/iOS/Webを単一コードベースで提供し、タブ（Detection / Labeling / Training / Models / Analytics）に機能を整理する。バックエンドはFastAPIで統一し、検出・語彙管理・学習・履歴・可視化・データ分析のAPIを備える。ユーザは語彙を自ら定義・追加し、必要データを小刻みに収集・注釈付けして学習をトリガし、結果を見ながら再収集・再学習を繰り返す。

\section{YOLO-Worldの運用}
YOLO-Worldは視覚−言語モデリングにより、ユーザ定義語彙での開放語彙検出を実現する。\texttt{POST /model/classes}で登録した語彙は\texttt{custom\_vocab.json}に永続化され、モデルのクラス埋め込みへ反映される。推論は\texttt{/detect}で実行し、バウンディングボックス・クラス・スコアと描画済み画像を返す。事前語彙化により、推論時はオフラインで固定した語彙埋め込みを用い、モバイルでも実用的なレイテンシを確保する。

\subsection{APIエンドポイントの概要}
\begin{table}[H]\centering
\setlength{\tabcolsep}{4pt}
\small
\begin{tabular}{p{0.22\linewidth}p{0.24\linewidth}p{0.48\linewidth}}
\toprule
Endpoint & 入力(要旨) & 主用途 \\
\midrule
/model/classes & クラス名(配列/文字列) & 語彙の登録・永続化（custom\_vocab.json） \\
/detect & 画像 & 事前語彙化済みHeadでの推論（BBox, Score, Class） \\
/detect/with-confidence & 画像, conf & 閾値指定推論（FP/FNバランス調整） \\
/labeling/submit & 画像, YOLOラベル & ラベル保存（images/labels, classes.txt更新） \\
/training/start & epochs等 & 同期学習の起動 \\
/training/start-async & epochs等 & 非同期学習の起動（UIブロック回避） \\
/training/status & - & 学習進捗・状態の取得 \\
/training/history & - & 学習履歴一覧の取得 \\
/training/metrics/\{run\_name\} & - & 時系列メトリクス取得（Plotly描画に利用） \\
/models/* & - & モデル一覧・切替・バックアップ・検証 \\
\bottomrule
\end{tabular}
\caption{主要APIエンドポイントの主用途}
\end{table}

\section{データ収集・ラベリング・学習}
Labelingタブで作成したアノテーションはYOLO形式で保存し（\texttt{training\_data/} 配下）、\texttt{/training/start}または\texttt{/training/start-async}で微調整を起動する。既定はCPU実行だが、CUDA対応マシンでは設定によりGPU（device=`cuda'）で学習・推論が可能である。完了時には\texttt{best.pt}を自動ロードし、\texttt{/training/status}で進捗を可視化する。\texttt{/models/*}群でモデル一覧・切替・バックアップ・検証が可能で、\texttt{/training/history}と\texttt{/training/metrics/\{run\_name\}}から学習履歴・時系列メトリクスを取得しUIでPlotly描画する。

\section{UIフロー（スマホ中心）と操作例}
本節では、スマートフォン想定の最短反復ループ（撮影→検出→語彙追加→再検出→ラベリング→学習→評価）を画面遷移で示す。各ページに4枚ずつ配置する。
\noindent 本図は，撮影→初回検出→語彙確認/追加の最短ループを示す。(a)で入力し，(b)で未検出を確認，(c)(d)で語彙追加により次ステップの検出改善を準備する。この操作により，学習前でも検出の立ち上がりを確認できる。

\begin{figure}[p]
\centering
\subfigure[撮影/選択（Detection）]{\includegraphics[width=0.48\linewidth]{Image_Goto/01_take_photo.png}}
\subfigure[初回検出（未学習）]{\includegraphics[width=0.48\linewidth]{Image_Goto/02_0_no_detect.png}}

\subfigure[クラス追加後の検出（既存クラス）]{\includegraphics[width=0.48\linewidth]{Image_Goto/02_1_detect.png}}
\subfigure[スマートフォンクラスの追加（Models）]{\includegraphics[width=0.48\linewidth]{Image_Goto/02_2_add_class.png}}
\caption[提案UIの反復ループ（1/4）]{提案UIの反復ループ（1/4）：撮影→初回検出→語彙確認/追加}
\label{fig:flow_page1}
\end{figure}

\begin{figure}[p]
\centering
\noindent 本図は，語彙追加後〜学習起動までの操作を示す。(a)語彙追加後の検出確認，(b)マニュアルラベリング，(c)学習パラメータ指定，(d)学習起動へ進む。\par
\subfigure[smartphoneクラスで検出成功]{\includegraphics[width=0.48\linewidth]{Image_Goto/02_3_detect_smartphone.png}}
\subfigure[マニュアルラベリング]{\includegraphics[width=0.48\linewidth]{Image_Goto/02_4_manual.png}}

\subfigure[ファインチューニング開始]{\includegraphics[width=0.48\linewidth]{Image_Goto/02_5_fine_tuning.png}}
\subfigure[学習の起動（Training）]{\includegraphics[width=0.48\linewidth]{Image_Goto/03_fine-tuning.png}}
\caption[提案UIの反復ループ（2/4）]{提案UIの反復ループ（2/4）：再検出→ラベリング→学習起動}
\label{fig:flow_page2}
\end{figure}

\begin{figure}[p]
\centering
\noindent 本図は，語彙・履歴・モデル管理の要点を示す。(a)語彙一覧，(b)学習履歴と指標，(c)モデル管理，(d)モデル概要の確認。これらにより改善仮説の検証が容易になる。\par
\subfigure[語彙・クラス一覧（Models）]{\includegraphics[width=0.48\linewidth]{Image_Goto/04_classes.png}}
\subfigure[学習履歴・指標の確認（Analytics）]{\includegraphics[width=0.48\linewidth]{Image_Goto/05_analytics.png}}

\subfigure[モデルの切替・管理]{\includegraphics[width=0.48\linewidth]{Image_Goto/06_model.png}}
\subfigure[モデル概要の確認]{\includegraphics[width=0.48\linewidth]{Image_Goto/07_model_overview.png}}
\caption[提案UIの反復ループ（3/4）]{提案UIの反復ループ（3/4）：語彙・履歴・モデル管理}
\label{fig:flow_page3}
\end{figure}

\begin{figure}[p]
\centering
\noindent 本図は，データ/性能の確認から次反復へ進む流れを示す。(a)データセット構成，(b)性能推移の把握，(c)(d)再掲により前段との対応付けを明確化する。\par
\subfigure[データセット確認]{\includegraphics[width=0.48\linewidth]{Image_Goto/08_model_dataset.png}}
\subfigure[性能比較・推移の確認]{\includegraphics[width=0.48\linewidth]{Image_Goto/09_model_performance.png}}

\subfigure[]{\includegraphics[width=0.48\linewidth]{Image_Goto/02_1_detect.png}} % 補助枠（統一のため再掲）
\subfigure[]{\includegraphics[width=0.48\linewidth]{Image_Goto/02_4_manual.png}} % 補助枠（統一のため再掲）
\caption[提案UIの反復ループ（4/4）]{提案UIの反復ループ（4/4）：データ・性能の確認と再反復}
\label{fig:flow_page4}
\end{figure}
\clearpage

\section{スマホ最適化と制約}
\begin{itemize}
  \item 推論効率: 事前語彙化によりテキストエンコーダを実行時から排除し、端末上のレイテンシを低減。
  \item 操作負荷の低減: 撮影→検出→語彙追加→学習の短サイクルをUIで誘導し、少量データでも改善可能に。
\item 制約: 既定はCPU学習であり大規模学習は長時間を要する。一方でCUDA対応マシンでは設定によりGPU学習・推論へ切替可能であり、反復時間を短縮できる。現状は学習/検証分割を簡便化しており、厳密評価は今後の拡張で対応する。
\end{itemize}

以上より、ユーザ主導の反復改善（語彙設定→収集/ラベリング→学習→推論/評価→運用）を一つのUIに束ね、スマートフォンを中心とした現場適用に耐える軽量な開放語彙検出運用を実現する。


\chapter{語彙登録のみでの検出成立性の検証}
\section{目的}
本章では、語彙登録（Add New Detection Class）のみで未検出の対象が検出可能になるかをスマートフォンから検証する。モデルはYOLO-Worldの事前語彙化運用であり、UI上の語彙追加が\texttt{POST /model/classes}に対応、検出は\texttt{POST /detect}に対応する。対象例として\texttt{smartphone}クラスを用い、Home → Capture Image → Add New Detection Class → Detect の最短ループで成立性を確認する（図\ref{fig:flow_page1}, 図\ref{fig:flow_page2}参照）。

\section{操作フローと条件}
\begin{enumerate}
  \item Homeでカメラ撮影またはギャラリーから画像選択（初回検出を実行し、対象が未検出であることを確認）。
  \item 入力欄に新規クラス名（例:\texttt{smartphone}）を入力し、Add New Detection Classを押下（\texttt{POST /model/classes}）。
  \item 直後に同一画像で再度検出（\texttt{POST /detect}）。語彙が反映され、対象が検出されるかを記録する。
\end{enumerate}
語彙は\texttt{custom\_vocab.json}に永続化され、アプリ再起動後も維持される。重複語彙は自動で除外され、空白のみの入力は無効化される。

\section{評価設定}
本章のゼロショット検証は以下の設定で行う。
\begin{table}[H]\centering
\begin{tabular}{ll}
\toprule
項目 & 設定 \\
\midrule
端末 & Xiaomi 13T Pro \\
入力解像度 & 長辺 1280 px（バッチ=1, スレッド=1） \\
confidence & 0.3（デフォルト。必要に応じて0.3–0.5で調整） \\
NMS閾値 & 0.5 \\
有効語彙 & \{smartphone\}（比較時は有効/無効の2条件） \\
モデル & YOLO-World（事前語彙化運用） \\
\bottomrule
\end{tabular}
\caption{ゼロショット検証の評価設定}
\end{table}

\section{評価項目}
\begin{itemize}
  \item 検出成立判定: 語彙追加前後での検出有無（バウンディングボックスとクラス名の一致）。
  \item 語彙反映時間: \texttt{POST /model/classes}送信から\texttt{/detect}結果に反映されるまでの体感時間（秒）。
  \item 推論レイテンシ: 画像1枚あたりの検出完了までの時間（UI表示基準；端末例: Android、入力解像度: 長辺1280px、バッチ=1、スレッド=1）。
\end{itemize}

\section{ケーススタディ（\texttt{smartphone}）}
スマートフォンの画像を対象に、初回は未検出であっても\texttt{smartphone}を語彙追加後に再検出すると検出成功するケースを確認した。UI例はAdd New Detection Classでの登録画面（図\ref{fig:flow_page1}）と、再検出での成立（図\ref{fig:flow_page2}）を参照。

\section{実験結果（\texttt{smartphone}10枚）}
語彙\texttt{smartphone}がある／ないの2条件で、同一の10枚画像（\texttt{assets/smartphone}由来）に対して\texttt{POST /detect}を実行した。追加学習は行っていない。\par
結果: 表\ref{tab:ch4_vocab_only_smartphone}のとおり、語彙登録のみで\texttt{smartphone}の検出が全件成立した。
\begin{table}[htbp]
\centering
\begin{tabular}{lcc}
\toprule
条件 & 検出枚数 & 成立率 \\
\midrule
WITH（\texttt{smartphone}あり） & 10/10 & 100\% \\
WITHOUT（\texttt{smartphone}なし） & 0/10  & 0\% \\
\bottomrule
\end{tabular}
\caption{語彙登録の有無による\texttt{smartphone}検出成立（10枚）}
\label{tab:ch4_vocab_only_smartphone}
\end{table}
検出例: 以下にWITHOUT/ WITHの比較（左: 語彙なし, 右: 語彙あり）を示す。語彙\texttt{smartphone}を追加することで、右図のように検出が成立している。\par
\begin{figure}[p]
\centering
\subfigure[1: without]{\includegraphics[width=0.45\linewidth]{Image_Goto/ch4_smartphone_wo_01.jpg}}
\subfigure[1: with]{\includegraphics[width=0.45\linewidth]{Image_Goto/ch4_smartphone_01.jpg}}
\\[0.5em]
\subfigure[2: without]{\includegraphics[width=0.45\linewidth]{Image_Goto/ch4_smartphone_wo_02.jpg}}
\subfigure[2: with]{\includegraphics[width=0.45\linewidth]{Image_Goto/ch4_smartphone_02.jpg}}
\caption[語彙\texttt{smartphone}の有無による比較（1/5）]{語彙\texttt{smartphone}の有無による比較（1/5）。
(a)(c) 語彙未登録（WITHOUT）：対象周辺に矩形状パターンがあるがスコアが閾値未満で検出不成立。
(b)(d) 語彙登録後（WITH）：エッジとテクスチャが語彙埋め込みと整合し検出成立。
反射が強い場合はスコアの揺らぎが残るが，confidenceを0.4以上に設定すると一部の誤検出（例：キーボード）は低減した。}
\label{fig:ch4_smartphone_results_cmp_1}
\end{figure}

\noindent 図\ref{fig:ch4_smartphone_results_cmp_1}(a)では反射によりエッジが不明瞭となり候補が閾値下で棄却される。一方(b)では語彙登録により類似度が上がり検出が成立する。図\ref{fig:ch4_smartphone_results_cmp_2}(d)ではキーボードの矩形パターンが誤検出を誘発しており，conf=0.4以上で当該FPが低減した（観察ベース）。

\begin{figure}[p]
\centering
\subfigure[3: without]{\includegraphics[width=0.45\linewidth]{Image_Goto/ch4_smartphone_wo_03.jpg}}
\subfigure[3: with]{\includegraphics[width=0.45\linewidth]{Image_Goto/ch4_smartphone_03.jpg}}
\\[0.5em]
\subfigure[4: without]{\includegraphics[width=0.45\linewidth]{Image_Goto/ch4_smartphone_wo_04.jpg}}
\subfigure[4: with]{\includegraphics[width=0.45\linewidth]{Image_Goto/ch4_smartphone_04.jpg}}
\caption[語彙\texttt{smartphone}の有無による比較（2/5）]{語彙\texttt{smartphone}の有無による比較（2/5）。(a)(c) 未登録では反射・陰影により境界が曖昧で不成立。(b)(d) 登録後は一貫して成立。低照度ではスコア低下がみられ，conf=0.4で安定化。}
\label{fig:ch4_smartphone_results_cmp_2}
\end{figure}
\noindent 図\ref{fig:ch4_smartphone_results_cmp_2}では，反射・低照度が主因で未登録時は不成立。登録後は成立し，conf上げで安定化が確認できる。

\begin{figure}[p]
\centering
\subfigure[5: without]{\includegraphics[width=0.45\linewidth]{Image_Goto/ch4_smartphone_wo_05.jpg}}
\subfigure[5: with]{\includegraphics[width=0.45\linewidth]{Image_Goto/ch4_smartphone_05.jpg}}
\\[0.5em]
\subfigure[6: without]{\includegraphics[width=0.45\linewidth]{Image_Goto/ch4_smartphone_wo_06.jpg}}
\subfigure[6: with]{\includegraphics[width=0.45\linewidth]{Image_Goto/ch4_smartphone_06.jpg}}
\caption[語彙\texttt{smartphone}の有無による比較（3/5）]{語彙\texttt{smartphone}の有無による比較（3/5）。(a)(c) スケール変化と部分遮蔽で候補が閾値未満。(b)(d) 登録後は小物体でも成立するが，極小スケールでは見逃しが残る。}
\label{fig:ch4_smartphone_results_cmp_3}
\end{figure}
\noindent 図\ref{fig:ch4_smartphone_results_cmp_3}では，小スケールと遮蔽が難例。登録後も極小対象は検出信頼度が低く追加データが有効。

\begin{figure}[p]
\centering
\subfigure[7: without]{\includegraphics[width=0.45\linewidth]{Image_Goto/ch4_smartphone_wo_07.jpg}}
\subfigure[7: with]{\includegraphics[width=0.45\linewidth]{Image_Goto/ch4_smartphone_07.jpg}}
\\[0.5em]
\subfigure[8: without]{\includegraphics[width=0.45\linewidth]{Image_Goto/ch4_smartphone_wo_08.jpg}}
\subfigure[8: with]{\includegraphics[width=0.45\linewidth]{Image_Goto/ch4_smartphone_08.jpg}}
\caption[語彙\texttt{smartphone}の有無による比較（4/5）]{語彙\texttt{smartphone}の有無による比較（4/5）。(a)(c) 背景の矩形パターンが類似外観として干渉。(b)(d) 登録後は成立するが，背景依存の誤検出が散見される。}
\label{fig:ch4_smartphone_results_cmp_4}
\end{figure}
\noindent 図\ref{fig:ch4_smartphone_results_cmp_4}では，背景の矩形パターンが誤検出の温床。語彙の絞り込みとconf調整で低減可能。

\begin{figure}[p]
\centering
\subfigure[9: without]{\includegraphics[width=0.45\linewidth]{Image_Goto/ch4_smartphone_wo_09.jpg}}
\subfigure[9: with]{\includegraphics[width=0.45\linewidth]{Image_Goto/ch4_smartphone_09.jpg}}
\\[0.5em]
\subfigure[10: without]{\includegraphics[width=0.45\linewidth]{Image_Goto/ch4_smartphone_wo_10.jpg}}
\subfigure[10: with]{\includegraphics[width=0.45\linewidth]{Image_Goto/ch4_smartphone_10.jpg}}
\caption[語彙\texttt{smartphone}の有無による比較（5/5）]{語彙\texttt{smartphone}の有無による比較（5/5）。(a)(c) 視点変化が大きい場合は特徴の一致が弱く不成立。(b)(d) 登録後は視点変化に一定の頑健性を示す。}
\label{fig:ch4_smartphone_results_cmp_5}
\end{figure}
\noindent 図\ref{fig:ch4_smartphone_results_cmp_5}では，視点変化が大きい画像で信頼度が揺らぐ。データ多様化（角度・距離）で改善余地。
\clearpage

\section{アブレーション：confidence閾値の影響}
\begin{table}[H]\centering
\begin{tabular}{lccc}
\toprule
conf & 成立率(10枚) & FP/画像(概数) & 備考 \\
\midrule
0.30 & 1.00 & 0.35 & 立ち上がり重視 \\
0.40 & 1.00 & 0.25 & バランス良 \\
0.50 & 0.90 & 0.15 & FP低減・FN増 \\
\bottomrule
\end{tabular}
\caption{confidence閾値の影響（観察ベース）}
\end{table}

\section{考察と制約}
本章の\texttt{smartphone}実験では、語彙登録のみで10/10枚の検出成立を確認した。一方で、少数の誤検出（False Positive: FP）や重複検出が見られるケースがあり、概ね検出できているが万能ではないという実務的な感触を得た。以下に観察と改善方針を整理する。
\begin{itemize}
  \item 成立性: 語彙\texttt{smartphone}の追加だけで、未検出から検出へと挙動が切り替わる。YOLO-Worldの事前語彙化により、\texttt{POST /model/classes}直後の\texttt{/detect}で反映が確認でき、UI上の操作で反復が容易。
  \item 誤検出の傾向: 画面の反射や光沢、矩形に近い背景パターン、他機器の類似外観で低スコアのFPが稀に発生。部分的に写るスマホ（遮蔽・極端なスケール変化）では検出の不安定化が起きやすい。また、今回ではキーボードの一部が写り込んでいる画像ではキーボードをsmartphoneと誤検出している。
  \item WITH/ WITHOUT比較の示唆: WITHOUTでは0/10、WITHでは10/10という実用上の差が大きく、語彙登録が最初の立ち上げ策として有効。ただし、誤検出抑制や難条件対応は語彙追加だけでは不十分な場合がある。
\end{itemize}
\noindent 改善方針:
\begin{itemize}
  \item 高性能モデルの利用: YOLO-Worldのモデルサイズが大きい高性能モデルを利用することで、誤検出を減らすことができる可能性がある。UI側でモデルを選択できるようにする。
  \item 閾値調整: \texttt{POST /detect/with-confidence?confidence=\{c\}}で信頼度閾値を上げるとFPが減る一方、見逃し（FN）が増えるため、0.3〜0.5を基準に対象や端末で調整する。
  \item 語彙の絞り込み: 同時に有効化する語彙を必要最小限に限定し、近縁語・紛らわしい語の同時登録を避けると混同が減る。クラス命名を一意で具体的に保つと安定しやすい。
  \item UI側の対策（confidence調整）: スライダ/ステップボタンで\texttt{confidence}を即時変更し、端末ごと/クラスごとの既定値を記憶（ローカル）する。低スコア候補は枠色を薄く、トップ1のみ表示やヒステリシス（表示の入り切りに差を持たせる）でちらつきを抑える。
  \item ユーザフィードバック（今後の拡張）: 検出枠ごとに正しい/誤検出のワンタップ評価、長押しで誤検出タグ付け、見逃し箇所のタップ追加などの軽量フィードバックを収集。正例/負例を自動エクスポートして保存し、一定数たまったら非同期トレーニングを提案する。難例キューを作り、Hard Negative重点微調整でFP低減を狙う。
\end{itemize}
総じて、語彙登録だけで「使い始められる」ことは本システムの強みであり、概ね期待どおりに検出が立ち上がる。一方で、少数の誤検出や難条件の安定性は残課題であり、閾値調整→データ増し→軽い微調整の順で段階的に品質を高めるのが実務的である。


%-------4章--------------------------------------------------------
\chapter{手動ラベリングとファインチューニングによる検出成立性の検証}
\section{目的}
語彙登録（4章）だけでは検出できなかった対象に対し、スマホからの手動ラベリングと短時間のファインチューニングで検出が成立するかを検証する。対象はUI上のManual Labeling（\texttt{POST /labeling/submit}）とTraining（\texttt{POST /training/start-async}）を用いた最短反復で評価する。

\section{前提と環境}
フロントエンドはReact Native + Expo、バックエンドはFastAPI。学習は既定でCPUだが、環境設定によりGPUへ切替可能。収集した画像とYOLO形式ラベルは\texttt{training\_data/images}・\texttt{training\_data/labels}へ保存され、クラス一覧は\texttt{training\_data/classes.txt}で管理される。学習設定\texttt{data.yaml}はAPIが自動生成する。

\section{手順}
\begin{enumerate}
  \item Homeで検出を実行し、目標対象が未検出であることを確認。
  \item Manual Labelingで矩形とラベル名を付与しSubmit（\texttt{/labeling/submit}）。これを少量ずつ（例:10〜50枚）蓄積。
  \item Training画面から非同期学習を起動（\texttt{/training/start-async?epochs=E}）。\texttt{E}は端末状況に応じて調整（例:20/50/100）。
  \item \texttt{/training/status}で進捗を確認。完了時に最良重み\texttt{best.pt}が自動ロードされる。
  \item Homeに戻り同一/類似画像で再検出し、検出成立の有無を確認。
\end{enumerate}

\section{評価設計}
\begin{itemize}
  \item 検出成立率: 学習前後での検出有無の比較（正しくバウンディング・分類できた割合）。
  \item 必要ラベル数の目安: 初回の検出成立に必要だったラベル枚数の概算。
  \item 反復時間: ラベリング→学習→検証の1サイクル所要時間（端末体感）。
  \item 副作用の観察: 語彙衝突・誤検出の増減（類義語追加時など）。
\end{itemize}

\section{実装上の要点}
\begin{itemize}
  \item ラベル保存: 送信データはYOLO形式で保存され、クラスは\texttt{classes.txt}へ自動追記。新規クラスは\texttt{/model/classes}へ同期される。
  \item 学習設定: \texttt{data.yaml}はAPIが自動生成。簡便化のため学習/検証は同一ディレクトリだが、将来的に分割を厳密化予定。
  \item モデル反映: 学習完了後、最良重みを自動ロード。\texttt{Models}タブで一覧・切替・バックアップ・簡易検証が可能。
\end{itemize}

\section{実験結果}
\subsection{実験設定}
本実験では、マウス（mouse）を対象として、語彙登録のみでは検出できなかったケースに対してファインチューニングによる検出成立性を検証した。実験環境はCPU（AMD Ryzen 5 4500 6-Core Processor）で実行し、学習データとして50枚の画像にYOLO形式のラベルを付与した。学習はEpoch 100で実施し、Early Stopping（patience=10）により38エポックで最良モデルが保存された。テスト画像として13枚の未学習画像を使用した。データ分割はtrain:val=8:2、乱数シードは42に固定した。

\subsection{実験手順}
\begin{enumerate}
  \item 初期状態でクラス追加なしのモデル（\texttt{yolov8s-world.pt}）で検出を試行し、マウスが検出されないことを確認。
  \item 50枚の画像に対して手動ラベリングを実施し、\texttt{training\_data/images}と\texttt{training\_data/labels}に保存。
  \item \texttt{/training/start-async?epochs=100}で非同期学習を起動。CPU環境で約34秒/エポックの速度で学習を実行。
  \item Early Stoppingにより38エポックで学習が停止し、最良重み（28エポック時）が\texttt{best.pt}として保存された。
  \item クラス追加を行わずに、ファインチューニング済みモデルを直接ロードして検出を試行。
\end{enumerate}

\subsection{学習結果}
学習は38エポックで完了し、最良モデルは28エポック時に記録された。最終的な評価指標は以下の通りである。

\begin{itemize}
  \item \textbf{mAP50}: 0.9235
  \item \textbf{mAP50-95}: 0.5188
  \item \textbf{Precision}: 0.9065
  \item \textbf{Recall}: 0.7754
\end{itemize}

学習時間は約0.453時間（約27分）であり、32枚のデータでEpoch 50を実行した前回実験と比較して、データ量の増加とEarly Stoppingにより効率的に学習が完了した。

\subsection{検出結果}
テスト画像13枚に対する検出結果を表\ref{tab:detection_results}に示す。クラス追加なしでファインチューニング済みモデルをロードした場合、モデル内部にクラス情報（\texttt{\{0: 'Chopsticks', 1: 'egg', 2: 'Soup', 3: 'Rice', 4: 'mouse'\}}）が埋め込まれていることを確認した。直接モデルの\texttt{predict}メソッドを呼び出すことで、検出が成立することを確認した。

\begin{table}[htbp]
\centering
\caption{ファインチューニング後の検出結果（テスト画像13枚、50枚学習、38エポック）}
\label{tab:detection_results}
\begin{tabular}{lcc}
\hline
画像名 & 検出数 & 最高信頼度 \\
\hline
mouse\_1.jpg & 0 & - \\
mouse\_2.jpg & 2 & 0.269 \\
mouse\_3.jpg & 0 & - \\
mouse\_4.jpg & 2 & 0.525 \\
mouse\_5.jpg & 1 & 0.489 \\
mouse\_6.jpg & 1 & 0.503 \\
mouse\_7.jpg & 0 & - \\
mouse\_8.jpg & 1 & 0.601 \\
mouse\_9.jpg & 1 & 0.435 \\
mouse\_10.jpg & 1 & 0.358 \\
mouse\_11.jpg & 1 & 0.332 \\
mouse\_12.jpg & 0 & - \\
mouse\_13.jpg & 1 & 0.279 \\
\hline
合計 & 11件 & - \\
検出率 & 69.2\% & - \\
\hline
\end{tabular}
\end{table}

\subsection{考察}
実験結果から以下の知見が得られた。

\begin{itemize}
  \item \textbf{ファインチューニングによるクラス情報の埋め込み}: ファインチューニング済みモデル（\texttt{best.pt}）には、学習時に使用したクラス情報がモデル内部に埋め込まれている。これにより、クラス追加（\texttt{/model/classes}）を行わなくても、モデル自体が検出可能なクラスを保持している。これは、YOLO-Worldの開放語彙検出の特性を活用しつつ、特定クラスに対する検出精度を向上させるファインチューニングの有効性を示している。

  \item \textbf{データ量増加による検出率向上}: 学習データを32枚から50枚に増加させた結果、検出率が40\%から69.2\%に向上した。これは、学習データの多様性が増加し、モデルの汎化性能が向上したことを示している。また、mAP50が0.9235と高い値を示しており、学習データに対する検出精度は十分に高い。

  \item \textbf{Early Stoppingの効果}: Early Stopping（patience=10）により、38エポックで学習が停止し、最良モデルは28エポック時に記録された。これにより、過学習を抑制しつつ、効率的に学習を完了できた。学習時間も約27分と短縮され、実用性が向上した。

  \item \textbf{検出率と信頼度の関係}: 検出できた画像の信頼度は0.279〜0.601の範囲であり、一部の画像では信頼度が低い（0.25〜0.35程度）。これは、テスト画像の撮影条件や角度が学習データと異なる場合に、検出が困難になることを示している。より高い検出率を達成するには、学習データの多様性をさらに向上させる必要がある。

  \item \textbf{実装上の制約と改善の余地}: 現在の実装では、\texttt{predict\_image}メソッドが\texttt{if not self.current\_classes:}のチェックを行っているため、通常のAPI経由（\texttt{/detect}）ではクラス追加なしでは検出できない。しかし、モデル内部にはクラス情報が含まれているため、実装を改善すればクラス追加なしでも検出可能である。これにより、ファインチューニング済みモデルの利便性がさらに向上する。

  \item \textbf{学習データとテストデータの分布}: 検出できなかった画像（4枚）は、学習データと異なる撮影条件や角度や背景、通常とは異なるマウスの製品外観を持つ可能性がある。より高い検出率を達成するには、学習データの多様性をさらに向上させ、様々な撮影条件をカバーする必要がある。
\end{itemize}

以上より、語彙登録だけでは検出できなかった対象に対し、少量のラベリングデータ（50枚）とファインチューニング（38エポック、Early Stopping適用）により、69.2\%の検出率を達成できることが確認された。ファインチューニング済みモデルにはクラス情報が埋め込まれており、クラス追加なしでも検出可能であることが実証された。ただし、より高い検出率を達成するには、学習データの多様性をさらに向上させる必要がある。

\section{期待と限界}
語彙登録だけで立ち上がらない対象でも、少量ラベル＋短時間学習で検出が成立することを期待する。一方で、撮影条件や外観多様性が大きい場合は追加データ収集と反復学習が必要。モバイル中心運用のため、学習/推論時間と電力の制約がある。

\section{今後の改善}
データ分割の厳密化、GPU環境での高速化、半自動ラベリング支援、同義語正規化、学習履歴の可視化充実（\texttt{/training/history}, \texttt{/training/metrics/\{run\_name\}}の活用）を進める。


%-------5章--------------------------------------------------------

\chapter{まとめ}
本稿では、一般ユーザがスマートフォンを含む汎用端末のみで、データ収集からラベリング、学習、評価、運用までを一気通貫に反復できる実用的な画像認識モデル運用基盤を構築した。中核には開放語彙検出器であるYOLO-Worldを据え、事前語彙化によって実行時の言語エンコーダ依存を排除し、軽量・高速な推論を維持する設計とした。

フロントエンドはReact Native + ExpoによりAndroid/iOS/Webを単一コードベースで提供し、Detection / Labeling / Training / Models / Analyticsの各タブに機能を整理した。バックエンドはFastAPIで統一し、検出・語彙管理・学習・履歴・可視化・データ分析のAPIを備えた。ユーザは語彙を自ら定義・追加し、必要データを小刻みに収集・注釈付けして学習をトリガし、結果を見ながら再収集・再学習を繰り返すワークフローを実現した。

実験により、語彙登録のみでは検出できなかった対象に対し、手動ラベリングとファインチューニングにより検出が成立することを確認した。50枚の学習データでEpoch 100（Early Stoppingにより38エポックで停止）の学習を実施し、テスト画像13枚に対して69.2\%の検出率を達成した。学習結果として、mAP50が0.9235、mAP50-95が0.5188、Precisionが0.9065、Recallが0.7754を記録し、ファインチューニング済みモデルにはクラス情報が埋め込まれており、クラス追加なしでも検出可能であることを実証した。

一方で、検出できなかった画像（4枚）も存在し、学習データと異なる撮影条件や角度、背景を持つ可能性がある。より高い検出率を達成するには、学習データの多様性をさらに向上させ、様々な撮影条件をカバーする必要がある。また、現在の実装では、\texttt{predict\_image}メソッドがクラス追加のチェックを行っているため、通常のAPI経由ではクラス追加なしでは検出できない。しかし、モデル内部にはクラス情報が含まれているため、実装を改善すればクラス追加なしでも検出可能である。

今後の課題として、学習データの多様性向上、データ分割の厳密化（train/valの明確な分離）、GPU環境での高速化、半自動ラベリング支援、同義語正規化、学習履歴の可視化充実などが挙げられる。本システムにより、プログラミング経験が乏しいユーザでも少量の自前データから用途特化モデルを反復的に改善でき、画像認識活用の敷居を下げることができた。

\noindent なお、本実験は小規模プロトタイプであり，テスト画像は13枚に限られている。実運用を見据えた評価には，より大規模なデータセットと厳密な検証が必要である。
\noindent なお、本システムは単一のコードベースからiOS・Android・Web向けにビルドおよび公開が可能である。バックエンドのFastAPIサーバも、クラウド等にデプロイすることで外部からアクセス可能な形で運用でき、モバイル／Webクライアントからの利用に対応する。

\appendix
\section{実験端末の詳細}
\begin{itemize}
  \item 端末: Xiaomi 13T Pro (Android)\
  \item 備考: 実験の主要なゼロショット検証・推論を当端末で実施\
  \item 学習用PC: CPU: AMD Ryzen 5 4500 (6-Core), メモリ: 16 GB, OS: Linux (kernel 6.8.0-87-generic), GPU: なし（CPU学習）\
  \item 再現条件: 50枚, Epoch 100（Early Stopping: patience=10）, 38エポック, 学習時間 約27分\
\end{itemize}

\begin{acknowledge}
本研究を進めるにあたり、終始熱心なご指導を頂いた孟林教授に深く感謝いたします。また、本研究において手助けをしていただいた研究室の皆様に感謝の念が絶えません。本当にありがとうございました。
\end{acknowledge}
%
% 参考文献（内包）
\begin{thebibliography}{99}
\bibitem{yoloworld}
Cheng, T., Song, L., Ge, Y., Liu, W., Wang, X., Shan, Y.,
``YOLO-World: Real-Time Open-Vocabulary Object Detection,''
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. arXiv:2401.17270 (v2).
\url{https://arxiv.org/abs/2401.17270}

\bibitem{glip}
Li, J., Li, D., et al.,
``GLIP: Grounded Language-Image Pre-training for Open-Vocabulary Object Detection,''
CVPR, 2022. \url{https://arxiv.org/abs/2112.03857}

\bibitem{groundingdino}
Liu, S., Zeng, Z., et al.,
``Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection,''
arXiv:2303.05499, 2023. \url{https://arxiv.org/abs/2303.05499}

\bibitem{owlvit}
Minderer, M., et al.,
``Simple Open-Vocabulary Object Detection with Vision Transformers,''
CVPR, 2023. \url{https://arxiv.org/abs/2205.06230}

\bibitem{detic}
Zhou, X., et al.,
``Detic: Detecting Twenty-thousand Classes using Image-level Supervision,''
ECCV, 2022. \url{https://arxiv.org/abs/2201.02605}

\bibitem{regionclip}
Zhong, Z., et al.,
``RegionCLIP: Region-based Language-Image Pretraining,''
CVPR, 2022. \url{https://arxiv.org/abs/2112.09106}

\bibitem{clip}
Radford, A., et al.,
``Learning Transferable Visual Models From Natural Language Supervision,''
ICML, 2021. \url{https://arxiv.org/abs/2103.00020}

\bibitem{food101}
Bossard, L., Guillaumin, M., Van Gool, L.,
``Food-101 – Mining Discriminative Components with Random Forests,''
ECCV, 2014. \url{https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/}

\bibitem{foodseg103}
Wu, X., et al.,
``FoodSeg103 and UEC-FoodPix Complete: Datasets for Food Segmentation,''
arXiv:2107.13375, 2021. \url{https://arxiv.org/abs/2107.13375}

\bibitem{im2calories}
Meyers, A., et al.,
``Im2Calories: Toward an Automated System for Calorie Estimation Using Food Images,''
CVPR, 2015. \url{https://arxiv.org/abs/1512.03322}

\bibitem{nutrition5k}
Chen, M., et al.,
``Nutrition5k: Towards Automatic Nutritional Understanding of Food Images,''
CVPR, 2020. \url{https://arxiv.org/abs/2002.10509}

\end{thebibliography}
%

\end{document}