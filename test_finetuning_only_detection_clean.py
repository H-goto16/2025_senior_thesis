#!/usr/bin/env python3
"""
ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ã¿ã§æ¤œå‡ºã§ãã‚‹ã‹å®Ÿé¨“ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆã‚¯ãƒªãƒ¼ãƒ³ç‰ˆï¼‰
- custom_vocab.jsonã‚’ç„¡è¦–
- ã‚¯ãƒ©ã‚¹è¿½åŠ ãªã—
- ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
- æ¤œå‡ºã‚’è©¦ã™
"""

import sys
import os
import tempfile
import shutil
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'backend', 'src'))

from yolo.object_detection import YoloDetector
from pathlib import Path

def test_finetuning_only_detection_clean():
    """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ã¿ã§æ¤œå‡ºã§ãã‚‹ã‹ãƒ†ã‚¹ãƒˆï¼ˆã‚¯ãƒªãƒ¼ãƒ³ç‰ˆï¼‰"""
    print("=" * 80)
    print("ğŸ”¬ å®Ÿé¨“: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ã¿ã§æ¤œå‡ºã§ãã‚‹ã‹ï¼Ÿï¼ˆã‚¯ãƒªãƒ¼ãƒ³ç‰ˆï¼‰")
    print("=" * 80)
    print()

    # æœ€æ–°ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’æ¢ã™
    runs_dir = Path("/home/haruki-goto/workspace/lab/dish_detection/runs/detect")
    if not runs_dir.exists():
        print("âŒ runs/detect directory not found")
        return

    train_dirs = sorted([d for d in runs_dir.iterdir() if d.is_dir() and d.name.startswith('train')],
                       key=lambda x: x.stat().st_mtime, reverse=True)

    if not train_dirs:
        print("âŒ No training runs found")
        return

    latest_run = train_dirs[0]
    best_model = latest_run / "weights" / "best.pt"

    if not best_model.exists():
        print(f"âŒ Best model not found: {best_model}")
        return

    print(f"ğŸ“¦ Using trained model: {best_model}")
    print()

    # custom_vocab.jsonã‚’ä¸€æ™‚çš„ã«ãƒªãƒãƒ¼ãƒ ã—ã¦ã€ã‚¯ãƒ©ã‚¹æƒ…å ±ã‚’èª­ã¿è¾¼ã¾ãªã„ã‚ˆã†ã«ã™ã‚‹
    vocab_file = Path("backend/src/custom_vocab.json")
    vocab_backup = None

    if vocab_file.exists():
        vocab_backup = vocab_file.with_suffix('.json.backup')
        shutil.move(str(vocab_file), str(vocab_backup))
        print(f"ğŸ“ custom_vocab.jsonã‚’ä¸€æ™‚çš„ã«ãƒªãƒãƒ¼ãƒ : {vocab_backup}")
        print()

    try:
        # 1. ã‚¯ãƒ©ã‚¹è¿½åŠ ãªã—ã§ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ï¼ˆcustom_vocab.jsonãªã—ï¼‰
        print("Step 1: ã‚¯ãƒ©ã‚¹è¿½åŠ ãªã—ã§ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ï¼ˆcustom_vocab.jsonãªã—ï¼‰")
        print("-" * 80)
        detector = YoloDetector(vocab_file="nonexistent_vocab.json")  # å­˜åœ¨ã—ãªã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®š
        print(f"åˆæœŸã‚¯ãƒ©ã‚¹: {detector.get_current_classes()}")
        print()

        # 2. ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        print("Step 2: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰")
        print("-" * 80)
        detector.load_trained_model(str(best_model))
        print(f"ãƒ­ãƒ¼ãƒ‰å¾Œã®ã‚¯ãƒ©ã‚¹: {detector.get_current_classes()}")
        print()

        # 3. ãƒ¢ãƒ‡ãƒ«ã«ã‚¯ãƒ©ã‚¹æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
        print("Step 3: ãƒ¢ãƒ‡ãƒ«ã®ã‚¯ãƒ©ã‚¹æƒ…å ±ã‚’ç¢ºèª")
        print("-" * 80)
        try:
            # YOLOãƒ¢ãƒ‡ãƒ«ã«ã¯nameså±æ€§ãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹
            if hasattr(detector.model, 'names'):
                print(f"ãƒ¢ãƒ‡ãƒ«ã®nameså±æ€§: {detector.model.names}")
            if hasattr(detector.model, 'model') and hasattr(detector.model.model, 'names'):
                print(f"ãƒ¢ãƒ‡ãƒ«å†…éƒ¨ã®nameså±æ€§: {detector.model.model.names}")
        except Exception as e:
            print(f"ã‚¯ãƒ©ã‚¹æƒ…å ±ã®ç¢ºèªä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        print()

        # 4. ã‚¯ãƒ©ã‚¹è¿½åŠ ãªã—ã§æ¤œå‡ºã‚’è©¦ã™
        print("Step 4: ã‚¯ãƒ©ã‚¹è¿½åŠ ãªã—ã§æ¤œå‡ºã‚’è©¦ã™")
        print("-" * 80)
        test_image = Path("assets/mouse/test_mouse/mouse_3.jpg")

        if not test_image.exists():
            print(f"âŒ Test image not found: {test_image}")
            return

        print(f"ãƒ†ã‚¹ãƒˆç”»åƒ: {test_image}")
        print(f"ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹: {detector.get_current_classes()}")
        print()

        # predict_imageã®ãƒã‚§ãƒƒã‚¯ã‚’å›é¿ã™ã‚‹ãŸã‚ã€ç›´æ¥ãƒ¢ãƒ‡ãƒ«ã‚’å‘¼ã³å‡ºã™
        print("ç›´æ¥ãƒ¢ãƒ‡ãƒ«ã§æ¤œå‡ºã‚’è©¦ã™ï¼ˆpredict_imageã®ãƒã‚§ãƒƒã‚¯ã‚’å›é¿ï¼‰")
        print("-" * 80)
        try:
            # ãƒ¢ãƒ‡ãƒ«ã«ã‚¯ãƒ©ã‚¹æƒ…å ±ãŒã‚ã‚‹ã‹ç¢ºèª
            if hasattr(detector.model, 'names') and detector.model.names:
                print(f"âœ… ãƒ¢ãƒ‡ãƒ«ã«ã‚¯ãƒ©ã‚¹æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã¾ã™: {detector.model.names}")
                # ç›´æ¥predictã‚’å‘¼ã³å‡ºã™
                results = detector.model.predict(str(test_image), conf=0.25, device=detector.device, verbose=False)
                result = results[0]

                if hasattr(result, 'boxes') and result.boxes is not None and len(result.boxes) > 0:
                    print(f"âœ… æ¤œå‡ºæˆåŠŸï¼æ¤œå‡ºæ•°: {len(result.boxes)}")
                    for i, box in enumerate(result.boxes):
                        cls_id = int(box.cls[0])
                        conf = float(box.conf[0])
                        cls_name = result.names[cls_id] if hasattr(result, 'names') else f"class_{cls_id}"
                        print(f"  [{i+1}] {cls_name}: confidence={conf:.3f}")
                    print()
                    print("ğŸ‰ çµè«–: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ã¿ã§æ¤œå‡ºå¯èƒ½ï¼")
                    print("ç†ç”±: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã«ã¯ã‚¯ãƒ©ã‚¹æƒ…å ±ãŒåŸ‹ã‚è¾¼ã¾ã‚Œã¦ã„ã¾ã™")
                else:
                    print("æ¤œå‡ºçµæœ: æ¤œå‡ºãªã—")
            else:
                print("âŒ ãƒ¢ãƒ‡ãƒ«ã«ã‚¯ãƒ©ã‚¹æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
                print("çµè«–: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ã¿ã§ã¯æ¤œå‡ºã§ãã¾ã›ã‚“")
        except Exception as e:
            print(f"âŒ æ¤œå‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()

    finally:
        # custom_vocab.jsonã‚’å¾©å…ƒ
        if vocab_backup and vocab_backup.exists():
            shutil.move(str(vocab_backup), str(vocab_file))
            print(f"ğŸ“ custom_vocab.jsonã‚’å¾©å…ƒã—ã¾ã—ãŸ")

    print()
    print("=" * 80)

if __name__ == "__main__":
    test_finetuning_only_detection_clean()

