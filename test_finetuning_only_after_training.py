#!/usr/bin/env python3
"""
å­¦ç¿’å®Œäº†å¾Œã€ã‚¯ãƒ©ã‚¹è¿½åŠ ãªã—ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ã¿ã§æ¤œå‡ºãƒ†ã‚¹ãƒˆ
"""

import sys
import os
import tempfile
import shutil
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'backend', 'src'))

from yolo.object_detection import YoloDetector
from pathlib import Path
import json

def test_finetuning_only_detection():
    """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ã¿ã§æ¤œå‡ºã§ãã‚‹ã‹ãƒ†ã‚¹ãƒˆ"""
    print("=" * 80)
    print("ğŸ”¬ å®Ÿé¨“: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ã¿ã§æ¤œå‡ºã§ãã‚‹ã‹ï¼Ÿï¼ˆ50æšã€100ã‚¨ãƒãƒƒã‚¯ï¼‰")
    print("=" * 80)
    print()

    # æœ€æ–°ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’æ¢ã™
    runs_dir = Path("/home/haruki-goto/workspace/lab/dish_detection/runs/detect")
    if not runs_dir.exists():
        print("âŒ runs/detect directory not found")
        return

    train_dirs = sorted([d for d in runs_dir.iterdir() if d.is_dir() and d.name.startswith('train')],
                       key=lambda x: x.stat().st_mtime, reverse=True)

    if not train_dirs:
        print("âŒ No training runs found")
        return

    latest_run = train_dirs[0]
    best_model = latest_run / "weights" / "best.pt"

    if not best_model.exists():
        print(f"âŒ Best model not found: {best_model}")
        return

    print(f"ğŸ“¦ Using trained model: {best_model}")
    print()

    # custom_vocab.jsonã‚’ä¸€æ™‚çš„ã«ãƒªãƒãƒ¼ãƒ 
    vocab_file = Path("backend/src/custom_vocab.json")
    vocab_backup = None

    if vocab_file.exists():
        vocab_backup = vocab_file.with_suffix('.json.backup')
        shutil.move(str(vocab_file), str(vocab_backup))
        print(f"ğŸ“ custom_vocab.jsonã‚’ä¸€æ™‚çš„ã«ãƒªãƒãƒ¼ãƒ : {vocab_backup}")
        print()

    try:
        # ã‚¯ãƒ©ã‚¹è¿½åŠ ãªã—ã§ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
        print("Step 1: ã‚¯ãƒ©ã‚¹è¿½åŠ ãªã—ã§ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–")
        print("-" * 80)
        detector = YoloDetector(vocab_file="nonexistent_vocab.json")
        print(f"åˆæœŸã‚¯ãƒ©ã‚¹: {detector.get_current_classes()}")
        print()

        # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        print("Step 2: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰")
        print("-" * 80)
        detector.load_trained_model(str(best_model))
        print(f"ãƒ­ãƒ¼ãƒ‰å¾Œã®ã‚¯ãƒ©ã‚¹: {detector.get_current_classes()}")
        print()

        # ãƒ¢ãƒ‡ãƒ«ã®ã‚¯ãƒ©ã‚¹æƒ…å ±ã‚’ç¢ºèª
        print("Step 3: ãƒ¢ãƒ‡ãƒ«ã®ã‚¯ãƒ©ã‚¹æƒ…å ±ã‚’ç¢ºèª")
        print("-" * 80)
        model_names = None
        try:
            if hasattr(detector.model, 'names'):
                model_names = detector.model.names
                print(f"ãƒ¢ãƒ‡ãƒ«ã®nameså±æ€§: {model_names}")
        except Exception as e:
            print(f"ã‚¯ãƒ©ã‚¹æƒ…å ±ã®ç¢ºèªä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        print()

        # test_mouseã®10æšã§æ¤œå‡ºãƒ†ã‚¹ãƒˆ
        print("Step 4: test_mouseã®10æšã§æ¤œå‡ºãƒ†ã‚¹ãƒˆï¼ˆã‚¯ãƒ©ã‚¹è¿½åŠ ãªã—ï¼‰")
        print("-" * 80)
        test_dir = Path("assets/mouse/test_mouse")

        if not test_dir.exists():
            print(f"âŒ Test directory not found: {test_dir}")
            return

        image_files = sorted(list(test_dir.glob("*.jpg")) + list(test_dir.glob("*.png")))

        if not image_files:
            print(f"âŒ No images found in {test_dir}")
            return

        print(f"ãƒ†ã‚¹ãƒˆç”»åƒ: {len(image_files)}æš")
        print()

        results = []
        for i, image_path in enumerate(image_files, 1):
            print(f"[{i}/{len(image_files)}] Processing {image_path.name}...")

            # ç›´æ¥ãƒ¢ãƒ‡ãƒ«ã§æ¤œå‡ºï¼ˆpredict_imageã®ãƒã‚§ãƒƒã‚¯ã‚’å›é¿ï¼‰
            try:
                if model_names:
                    results_direct = detector.model.predict(str(image_path), conf=0.25, device=detector.device, verbose=False)
                    result = results_direct[0]

                    detections = []
                    if hasattr(result, 'boxes') and result.boxes is not None and len(result.boxes) > 0:
                        for box in result.boxes:
                            cls_id = int(box.cls[0])
                            conf = float(box.conf[0])
                            cls_name = result.names[cls_id] if hasattr(result, 'names') else f"class_{cls_id}"
                            bbox = [float(coord) for coord in box.xyxy[0].tolist()]

                            detections.append({
                                "class": cls_name,
                                "confidence": conf,
                                "bbox": bbox
                            })

                        print(f"  âœ… Found {len(detections)} detection(s)")
                        for det in detections:
                            print(f"     - {det['class']}: confidence={det['confidence']:.3f}")
                    else:
                        print(f"  âš ï¸  No detections")

                    results.append({
                        "image_name": image_path.name,
                        "image_path": str(image_path),
                        "detections": detections
                    })
                else:
                    print(f"  âŒ Model names not available")
                    results.append({
                        "image_name": image_path.name,
                        "image_path": str(image_path),
                        "detections": [],
                        "error": "Model names not available"
                    })
            except Exception as e:
                print(f"  âŒ Error: {e}")
                results.append({
                    "image_name": image_path.name,
                    "image_path": str(image_path),
                    "detections": [],
                    "error": str(e)
                })
            print()

        # ã‚µãƒãƒªãƒ¼
        print("=" * 80)
        print("ğŸ“Š DETECTION SUMMARY")
        print("=" * 80)

        total_detections = 0
        successful_images = 0
        failed_images = 0

        for result in results:
            image_name = result.get("image_name", "unknown")
            if "error" in result:
                failed_images += 1
                print(f"âŒ {image_name}: ERROR - {result.get('error', 'Unknown error')}")
            else:
                detections = result.get("detections", [])
                if detections:
                    successful_images += 1
                    total_detections += len(detections)
                    print(f"âœ… {image_name}: {len(detections)} detection(s)")
                    for det in detections:
                        print(f"   - {det['class']}: confidence={det['confidence']:.3f}")
                else:
                    failed_images += 1
                    print(f"âš ï¸  {image_name}: No detections")

        print()
        print("=" * 80)
        print(f"Total images: {len(results)}")
        print(f"Successful: {successful_images}")
        print(f"Failed: {failed_images}")
        print(f"Total detections: {total_detections}")
        print(f"Detection rate: {successful_images / len(results) * 100:.1f}%")
        print("=" * 80)

        # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        output_file = Path("detection_results_finetuning_only_50images_100epochs.json")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ Results saved to: {output_file}")

    finally:
        # custom_vocab.jsonã‚’å¾©å…ƒ
        if vocab_backup and vocab_backup.exists():
            shutil.move(str(vocab_backup), str(vocab_file))
            print(f"ğŸ“ custom_vocab.jsonã‚’å¾©å…ƒã—ã¾ã—ãŸ")

    print()
    print("=" * 80)

if __name__ == "__main__":
    test_finetuning_only_detection()

